{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf09131f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.util import ngrams\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b358a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_PATH = \"path_to_text_corpus\"\n",
    "MODEL_PATH=\"path_to_output_model\"\n",
    "\n",
    "CORPUS_DOCS_SEP = \"\\n\\n\"\n",
    "MAX_NGRAM_LEN = 3\n",
    "LOWERCASE = True\n",
    "LANGUAGE = \"english\"\n",
    "REMOVE_STOPWORDS = False\n",
    "STEM = False\n",
    "PROB_WORDS_LIMIT = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4127ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(MODEL_PATH):\n",
    "    raise FileNotFoundError(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c0dc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with open(CORPUS_PATH, \"r\", encoding=\"utf-8\") as fh:\n",
    "    docs = fh.read().split(CORPUS_DOCS_SEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8fb4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this by more sophisticated tokenizer depending on your demands\n",
    "\n",
    "if REMOVE_STOPWORDS:\n",
    "    STOPWORDS = set(stopwords.words(LANGUAGE))\n",
    "    \n",
    "if STEM:\n",
    "    STEMMER = SnowballStemmer(LANGUAGE)\n",
    "\n",
    "def tokenize(doc):\n",
    "    if LOWERCASE:\n",
    "        doc = doc.lower()\n",
    "        \n",
    "    tokens = re.split(r\"\\W+\", doc)\n",
    "    \n",
    "    if REMOVE_STOPWORDS:\n",
    "        tokens = [token for token in tokens if token not in STOPWORDS]\n",
    "        \n",
    "    if STEM:\n",
    "        tokens = [STEMMER.stem(token) for token in tokens]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9859267",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_stat = {}\n",
    "\n",
    "min_ngram_freq = max(5, len(docs) // 10**5)\n",
    "\n",
    "for ngram_len in range(2, MAX_NGRAM_LEN+1):\n",
    "    ngrams_stat[ngram_len] = Counter()\n",
    "    \n",
    "    for doc in tqdm(docs):\n",
    "        tokens = tokenize(doc)\n",
    "\n",
    "        for ngram in ngrams(tokens, ngram_len):\n",
    "            ngrams_stat[ngram_len][ngram] += 1\n",
    "\n",
    "    ngrams_stat[ngram_len] = [item for item in ngrams_stat[ngram_len].items() if item[1] >= min_ngram_freq]\n",
    "    \n",
    "del docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2430023",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ngram_len in range(2, MAX_NGRAM_LEN+1):\n",
    "    prob_words = {}\n",
    "    \n",
    "    for ngram, cnt in tqdm(ngrams_stat[ngram_len]):\n",
    "        for mask_pos in range(ngram_len):\n",
    "            context_tokens = tuple([ngram[i] for i in range(ngram_len) if i != mask_pos])\n",
    "\n",
    "            prob_words.setdefault(mask_pos, {})\n",
    "            prob_words[mask_pos].setdefault(context_tokens, {})\n",
    "            prob_words[mask_pos][context_tokens][ngram[mask_pos]] = cnt\n",
    "    \n",
    "    for mask_pos in range(ngram_len):\n",
    "        prob_words[mask_pos] = {k: v for k, v in prob_words[mask_pos].items() if len(v) > 1}\n",
    "         \n",
    "        for context_tokens in tqdm(prob_words[mask_pos]):\n",
    "            words = prob_words[mask_pos][context_tokens]\n",
    "            \n",
    "            cnt_sum = sum(words.values())\n",
    "            \n",
    "            words = {k: words[k] for k in sorted(words, key=lambda x: words[x], reverse=True)[:PROB_WORDS_LIMIT]}\n",
    "            \n",
    "            prob_words[mask_pos][context_tokens] = {k: float(words[k] / cnt_sum) for k in words}\n",
    "\n",
    "    with open(f\"{MODEL_PATH}/{ngram_len}grams.pkl\", \"wb\") as fh:\n",
    "        pickle.dump(prob_words, fh)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
